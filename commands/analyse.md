---
description: Auto-selects best Kaizen method (Gemba Walk, Value Stream, or Muda) for target
argument-hint: 分析対象の説明（オプション）
---

# スマート分析

分析対象に基づいて最適なKaizen分析手法を自動選択し適用する。

## 説明

コンテキストを分析し、最適な手法を選択: 現地現物（コード探索）、バリューストリームマッピング（ワークフロー/プロセス）、ムダ分析（無駄の特定）。選択した手法でガイドする。

## 使用方法

`/analyse [対象の説明]`

例:
- `/analyse 認証機能の実装`
- `/analyse デプロイワークフロー`
- `/analyse コードベースの非効率性`

## 変数

- TARGET: 分析対象（デフォルト: 入力を促す）
- METHOD: 自動選択のオーバーライド（gemba, vsm, muda）

## 手法選択ロジック

**現地現物（Gemba Walk）** → 以下を分析する場合:
- コード実装（機能が実際にどう動作するか）
- ドキュメントと実態のギャップ
- 不慣れなコードベース領域の理解
- 実際のアーキテクチャと想定の違い

**バリューストリームマッピング** → 以下を分析する場合:
- ワークフローとプロセス（CI/CD、デプロイ、開発）
- 多段階パイプラインのボトルネック
- チーム/システム間のハンドオフ
- 各プロセス段階での所要時間

**ムダ分析** → 以下を分析する場合:
- コード品質と効率性
- 技術的負債
- 過剰エンジニアリングや重複
- リソース使用率

## 手順

1. 分析対象を理解
2. 最適な手法を決定（または指定された手法を使用）
3. この手法が適切な理由を説明
4. 分析をガイド
5. アクショナブルな洞察とともに発見を提示

---

## 手法1: 現地現物（Gemba Walk）

想定と現実のギャップを理解するため、実際のコードを「見に行く」。

### 使用タイミング

- 機能が実際にどう動作するかを理解する
- コード考古学（レガシーシステム）
- ドキュメントと実装のギャップを見つける
- 変更前に不慣れな領域を探索する

### プロセス

1. **スコープを定義**: 探索するコード領域
2. **想定を述べる**: 何をするコードと思っているか
3. **現実を観察**: 実際のコードを読む
4. **発見を記録**:
   - エントリーポイント
   - 実際のデータフロー
   - 驚き（想定との違い）
   - 隠れた依存関係
   - ドキュメント化されていない動作
5. **ギャップを特定**: ドキュメント vs 実態
6. **推奨**: ドキュメント更新、リファクタリング、または現状維持

### 例: 認証システムの現地現物

```
スコープ: ユーザー認証フロー

想定（実施前）:
• JWTトークンはlocalStorageに保存
• OAuthのみでシングルサインオン
• セッションは1時間後に期限切れ
• パスワードリセットはメールリンク経由

現地現物の観察（実際のコード）:

エントリーポイント: /api/auth/login (routes/auth.ts:45)
├─> AuthService.authenticate() (services/auth.ts:120)
├─> UserRepository.findByEmail() (db/users.ts:67)
├─> bcrypt.compare() (services/auth.ts:145)
└─> TokenService.generate() (services/token.ts:34)

実際のフロー:
1. ログイン資格情報 → POST /api/auth/login
2. パスワードはbcryptでハッシュ化（10ラウンド）
3. JWTは24時間の有効期限で生成（1時間ではない！）
4. トークンはhttpOnly cookieに保存（localStorageではない）
5. リフレッシュトークンは別のcookie（15日間）
6. セッションデータはRedis（30日TTL）

驚き:
✗ OAuthは実装されていない（コメントアウトされたコード発見）
✗ パスワードリセットは手動（管理者介入）
✗ 3つの異なるセッション保存メカニズム:
  - セッションデータ用Redis
  - 「ログイン状態を保持」用データベース
  - トークン用Cookie
✗ レガシーエンドポイント /auth/legacy がまだ有効（認証なし！）
✗ 管理者ユーザーはレート制限をバイパス（セキュリティ問題）

ギャップ:
• ドキュメントはOAuthと言っているが、コードにはない
• セッション有効期限が不一致（ドキュメント: 1時間、コード: 24時間）
• レガシーエンドポイントがドキュメント化されていない（セキュリティリスク）
• 「ログイン状態を保持」についてドキュメントに記載なし

推奨:
1. HIGH: /auth/legacyエンドポイントを保護または削除
2. HIGH: 実際のセッション有効期限（24時間）をドキュメント化
3. MEDIUM: OAuthをクリーンアップまたは実装
4. MEDIUM: セッション保存を統合（1つを選択）
5. LOW: 管理者ユーザーにもレート制限を追加
```

### 例: CI/CDパイプラインの現地現物

```
スコープ: ビルドとデプロイパイプライン

想定:
• すべてのコミットで自動テストが実行される
• ステージングへのデプロイは自動
• 本番デプロイには承認が必要

現地現物の観察:

実際のパイプライン (.github/workflows/main.yml):
1. mainへのプッシュ時:
   ├─> Lint (2分)
   ├─> ユニットテスト (5分) [コミットに"[skip-tests]"があればスキップ]
   ├─> Dockerイメージビルド (15分)
   └─> ステージングにデプロイ (3分)

2. 本番用の手動トリガー:
   ├─> 統合テスト実行 (20分) [本番のみ！]
   ├─> セキュリティスキャン (10分)
   └─> 本番にデプロイ (5分)

驚き:
✗ コミットメッセージフラグでユニットテストをスキップ可能
✗ 統合テストは本番デプロイ時のみ実行
✗ ステージングは統合テストなしでデプロイ
✗ ロールバック機構がない（手動kubectlコマンド）
✗ シークレットは.envファイルから読み込み（シークレットマネージャーではない）
✗ 古い「hotfix」ブランチがすべてのチェックをバイパス

ギャップ:
• ステージングと本番で異なるテストカバレッジ
• ドキュメントにテストスキップフラグの記載なし
• ロールバックプロセスがドキュメント化または自動化されていない
• セキュリティスキャン結果が強制されていない（警告のみ）

推奨:
1. CRITICAL: テストスキップフラグ機能を削除
2. CRITICAL: シークレットをシークレットマネージャーに移行
3. HIGH: ステージングでも統合テストを実行
4. HIGH: hotfixブランチを削除または保護
5. MEDIUM: 自動ロールバック機能を追加
6. MEDIUM: セキュリティスキャンをブロッキングに
```

---

## 手法2: バリューストリームマッピング

ワークフロー段階をマップし、時間/無駄を測定し、ボトルネックを特定。

### 使用タイミング

- プロセス最適化（CI/CD、デプロイ、コードレビュー）
- 多段階ワークフローの理解
- 遅延とハンドオフの発見
- サイクルタイムの改善

### プロセス

1. **開始と終了を特定**: プロセスの開始と終了地点
2. **すべてのステップをマップ**: 待機/ハンドオフ時間を含む
3. **各ステップを測定**:
   - 処理時間（作業中）
   - 待機時間（アイドル、ブロック）
   - 実行者/システム
4. **メトリクスを計算**:
   - 総リードタイム
   - 付加価値時間 vs 無駄な時間
   - 効率性 %（付加価値 / 総時間）
5. **ボトルネックを特定**: 最長のステップ、最多の待機
6. **将来状態を設計**: 最適化されたフロー
7. **改善を計画**: 将来状態の達成方法

### 例: 機能開発のバリューストリームマップ

```
現状: 機能要求 → 本番

ステップ1: 要件収集
├─ 処理: 2日（ミーティング、仕様書作成）
├─ 待機: 3日（ステークホルダーレビュー）
└─ 担当: プロダクトマネージャー

ステップ2: 設計
├─ 処理: 1日（モックアップ、アーキテクチャ）
├─ 待機: 2日（デザインレビュー、フィードバック）
└─ 担当: デザイナー + アーキテクト

ステップ3: 開発
├─ 処理: 5日（コーディング）
├─ 待機: 2日（PRレビュー待ち）
└─ 担当: 開発者

ステップ4: コードレビュー
├─ 処理: 0.5日（レビュー）
├─ 待機: 1日（やり取りと修正）
└─ 担当: シニア開発者

ステップ5: QAテスト
├─ 処理: 2日（手動テスト）
├─ 待機: 1日（バグ修正、再テスト）
└─ 担当: QAエンジニア

ステップ6: ステージングデプロイ
├─ 処理: 0.5日（デプロイ、スモークテスト）
├─ 待機: 2日（ステークホルダーUAT）
└─ 担当: DevOps

ステップ7: 本番デプロイ
├─ 処理: 0.5日（デプロイ、監視）
├─ 待機: 0日
└─ 担当: DevOps

───────────────────────────────────────
メトリクス:
総リードタイム: 22.5日
付加価値時間: 11.5日（作業）
無駄な時間: 11日（待機）
効率性: 51%

ボトルネック:
1. 要件レビュー待ち（3日）
2. 開発時間（5日）
3. ステークホルダーUAT待ち（2日）
4. PRレビュー待ち（2日）

無駄分析:
• レビュー/承認待ち: 9日（無駄の82%）
• 不明確な要件による手戻り: 約1日
• 手動テスト時間: 2日

将来状態設計:

変更:
1. 非同期の要件承認（ステークホルダーは24時間SLA）
2. 大きな機能を小さな増分に分割
3. 自動テストで手動QAを置き換え
4. PRレビューSLA: 最大4時間
5. ステージングへの継続的デプロイ（承認不要）
6. 本番ロールアウトにフィーチャーフラグ（待機なし）

予測される将来状態:
総リードタイム: 9日（60%削減）
付加価値時間: 8日
無駄な時間: 1日
効率性: 89%

実装計画:
第1週: レビューSLA設定、フィーチャーフラグ追加
第2週: テストスイート自動化
第3週: ステージング継続デプロイ有効化
第4週: 増分デリバリーのチームトレーニング
```

### 例: インシデント対応のバリューストリームマップ

```
現状: インシデント検知 → 解決

ステップ1: 検知
├─ 処理: 0分（自動アラート）
├─ 待機: 15分（誰かがアラートに気づくまで）
└─ システム: 監視ツール

ステップ2: トリアージ
├─ 処理: 10分（重大度評価）
├─ 待機: 20分（適切な担当者を探す）
└─ 担当: オンコールエンジニア

ステップ3: 調査
├─ 処理: 45分（ログ、デバッグ）
├─ 待機: 30分（本番アクセス、コンテキスト収集）
└─ 担当: エンジニア + SRE

ステップ4: 修正開発
├─ 処理: 60分（修正作成）
├─ 待機: 15分（コードレビュー）
└─ 担当: エンジニア

ステップ5: デプロイ
├─ 処理: 10分（ホットフィックスデプロイ）
├─ 待機: 5分（検証）
└─ 担当: SRE

ステップ6: インシデント後
├─ 処理: 20分（ステータス更新、通知）
├─ 待機: 0分
└─ 担当: エンジニア

───────────────────────────────────────
メトリクス:
総リードタイム: 230分（3時間50分）
付加価値時間: 145分
無駄な時間: 85分（37%）

ボトルネック:
1. 適切な担当者を探す（20分）
2. 本番アクセス取得（30分）
3. 調査時間（45分）

改善:
1. アラートのSlack連携（検知待ち削減）
2. サービスオーナーによる自動アサイン（担当者探し不要）
3. オンコール用の事前承認済み本番アクセス（待機削減）
4. 一般的なインシデント用ランブック（調査高速化）
5. デプロイインシデント用の自動ロールバック

予測改善: 230分 → 120分（48%高速化）
```

---

## 手法3: ムダ分析

コードと開発プロセスにおける7種類の無駄を特定。

### 使用タイミング

- コード品質監査
- 技術的負債の評価
- プロセス効率の改善
- 過剰エンジニアリングの特定

### 7種類の無駄（ソフトウェアへの適用）

**1. 作りすぎ**: 必要以上に作る
- 誰も使わない機能
- 過度に複雑なソリューション
- 時期尚早な最適化
- 不要な抽象化

**2. 手待ち**: アイドル時間
- ビルド/テスト/デプロイ時間
- コードレビューの遅延
- 依存関係の待ち
- 他チームによるブロック

**3. 運搬**: 物を動かす
- 不要なデータ変換
- 付加価値のないAPIレイヤー
- システム間のデータコピー
- 繰り返しのシリアライズ/デシリアライズ

**4. 加工**: 必要以上の処理
- 過剰なログ
- 冗長なバリデーション
- 過度に正規化されたデータベース
- 不要な計算

**5. 在庫**: 仕掛品
- マージされていないブランチ
- 未完成の機能
- トリアージされていないバグ
- デプロイされていないコード

**6. 動作**: 不要な動き
- コンテキストスイッチ
- 目的のないミーティング
- 手動デプロイ
- 繰り返しタスク

**7. 不良**: 手戻りとバグ
- 本番バグ
- 技術的負債
- フレーキーテスト
- 不完全な機能

### プロセス

1. **スコープを定義**: コードベース領域またはプロセス
2. **各無駄の種類を検証**
3. **影響を定量化**（時間、複雑さ、コスト）
4. **影響度で優先順位付け**
5. **排除戦略を提案**

### 例: APIコードベースの無駄分析

```
スコープ: REST APIバックエンド（50K LOC）

1. 作りすぎ
   発見:
   • 使用実績ゼロのAPIエンドポイント15件（過去90日間）
   • 「将来の柔軟性」のために作られた汎用「フレームワーク」（未使用）
   • 時期尚早なマイクロサービス分割（2サービス、1つでよい）
   • 12機能のフィーチャーフラグ（10は完全ロールアウト済み、フラグ維持）

   影響: 8K LOCが理由なく保守されている
   推奨: 未使用エンドポイントを削除、古いフラグを削除

2. 手待ち
   発見:
   • CIパイプライン: 45分（遅いDockerビルド）
   • PRレビュー時間: 平均2日
   • ステージングへのデプロイ: 手動、1時間かかる

   影響: 機能あたり2.5日の無駄
   推奨: Dockerレイヤーキャッシュ、PRレビューSLA、ステージング自動化

3. 運搬
   発見:
   • DBからAPIレスポンスまでデータが4回変換:
     DB → ORM → Service → DTO → Serializer
   • リクエスト/レスポンスが3回ログ記録（ミドルウェア、ハンドラー、サービス）
   • ファイルアップロード → S3 → CloudFront → ローカルキャッシュ（不要）

   影響: 平均200msのレスポンス時間オーバーヘッド
   推奨: 変換レイヤー削減、ログ統合

4. 加工
   発見:
   • すべてのリクエストで認証トークン検証（キャッシュ済みでも）
   • データベースクエリがすべての列を取得（SELECT *）
   • JSONレスポンスが完全なオブジェクトグラフを含む（5レベルネスト）
   • 本番ですべてのデータベースクエリをログ（冗長）

   影響: 40%高いデータベース負荷、3倍のログストレージ
   推奨: 認証チェックをキャッシュ、選択的フィールド、レスポンス軽量化

5. 在庫
   発見:
   • 23件のオープンPR（8件放棄、6ヶ月以上経過）
   • 5件のマージされていない機能ブランチ（完了だがデプロイされていない）
   • 147件のオープンバグ（42件重複、60件再現不能）
   • mainにバックポートされていない12件のホットフィックスコミット

   影響: コンテキストオーバーヘッド、マージコンフリクト、作業の損失
   推奨: 古いPRをクローズ、バグトリアージ、保留中の機能をデプロイ

6. 動作
   発見:
   • 開発者が1回のデプロイで4つのツールを切り替え
   • 手動データベースマイグレーション（エラーが起きやすい、遅い）
   • 環境設定が6ファイルに分散
   • .envファイルへのシークレットコピーペースト

   影響: デプロイあたり30分、頻繁なミス
   推奨: 統一デプロイツール、マイグレーション自動化

7. 不良
   発見:
   • 月12件の本番バグ
   • 15%のフレーキーテスト率（リトライ時間の無駄）
   • 認証モジュールの技術的負債（リファクタリング必要）
   • 不完全なエラーハンドリング（グレースフルではなくクラッシュ）

   影響: 顧客からの苦情、手戻り、ダウンタイム
   推奨: テスト安定化、認証リファクタリング、エラーバウンダリ追加

───────────────────────────────────────
サマリー

特定された無駄の合計:
• コード: 8K LOCが何もしていない
• 時間: 機能あたり2.5日
• パフォーマンス: リクエストあたり200msオーバーヘッド
• 工数: デプロイあたり30分

優先修正（影響度順）:
1. HIGH: デプロイ自動化（動作 + 手待ち削減）
2. HIGH: フレーキーテスト修正（不良削減）
3. MEDIUM: 未使用コード削除（作りすぎ削減）
4. MEDIUM: データ変換最適化（運搬削減）
5. LOW: バグバックログトリアージ（在庫削減）

予測される回復:
• 20%高速な機能デリバリー
• 50%少ない本番問題
• 30%少ない運用オーバーヘッド
```

---

## 注意事項

- 手法選択はコンテキスト依存—最適なものを選ぶ
- 手法を組み合わせることも可能（現地現物 → ムダ分析）
- 不慣れな領域では現地現物から始める
- プロセス最適化にはVSMを使用
- 効率性とクリーンアップにはムダを使用
- すべての手法はアクショナブルな改善につながるべき
- 組織学習のために発見を記録
- 発見の包括的なドキュメント化には `/analyse-problem`（A3）の使用を検討
